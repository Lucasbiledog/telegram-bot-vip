#!/usr/bin/env python3
"""
Script Execut√°vel para Testes de Stress do Bot Telegram
Use este script para executar diferentes tipos de teste facilmente
"""

import asyncio
import sys
import argparse
import json
import time
from pathlib import Path

# Importar nossos m√≥dulos
try:
    from stress_test import StressTester
    from test_config import (
        StressTestConfig, get_config_for_environment,
        get_scenario_config, print_config_summary,
        validate_config, PERFORMANCE_THRESHOLDS
    )
    from performance_monitor import start_monitoring, stop_monitoring, get_performance_summary
except ImportError as e:
    print(f"Erro ao importar modulos: {e}")
    print("Certifique-se de que todos os arquivos est√£o no mesmo diret√≥rio")
    sys.exit(1)

def create_argument_parser():
    """Cria parser de argumentos de linha de comando"""
    parser = argparse.ArgumentParser(
        description="ü§ñ Teste de Stress para Bot Telegram VIP",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:

  # Teste r√°pido com 100 usu√°rios
  python run_stress_test.py --quick

  # Teste com cen√°rio predefinido
  python run_stress_test.py --scenario "Carga Alta"

  # Teste customizado
  python run_stress_test.py --users 500 --rps 50 --webhook http://localhost:8000/webhook

  # Teste em ambiente espec√≠fico
  python run_stress_test.py --environment production --scenario "Carga Baixa"

  # Teste com monitoramento avan√ßado
  python run_stress_test.py --users 1000 --rps 100 --monitor --export-metrics

Cen√°rios dispon√≠veis:
  ‚Ä¢ Carga Baixa: 100 usu√°rios, 10/s
  ‚Ä¢ Carga M√©dia: 500 usu√°rios, 50/s
  ‚Ä¢ Carga Alta: 1000 usu√°rios, 100/s
  ‚Ä¢ Stress Extremo: 2000 usu√°rios, 200/s
  ‚Ä¢ Spike Test: 1500 usu√°rios, 300/s
        """
    )

    # Argumentos principais
    parser.add_argument(
        "--users", "-u",
        type=int,
        help="N√∫mero total de usu√°rios falsos (padr√£o: 1000)"
    )

    parser.add_argument(
        "--rps", "-r",
        type=int,
        help="Requisi√ß√µes por segundo (padr√£o: 100)"
    )

    parser.add_argument(
        "--webhook", "-w",
        type=str,
        help="URL do webhook do bot (padr√£o: http://localhost:8000/webhook)"
    )

    parser.add_argument(
        "--duration", "-d",
        type=int,
        help="Dura√ß√£o m√°xima em segundos (padr√£o: 300)"
    )

    # Cen√°rios predefinidos
    parser.add_argument(
        "--scenario", "-s",
        type=str,
        choices=["Carga Baixa", "Carga M√©dia", "Carga Alta", "Stress Extremo", "Spike Test"],
        help="Usar cen√°rio predefinido"
    )

    parser.add_argument(
        "--environment", "-e",
        type=str,
        choices=["local", "development", "production"],
        default="local",
        help="Ambiente alvo (padr√£o: local)"
    )

    # Testes r√°pidos
    parser.add_argument(
        "--quick", "-q",
        action="store_true",
        help="Teste r√°pido (100 usu√°rios, 20/s)"
    )

    parser.add_argument(
        "--spike",
        action="store_true",
        help="Teste de pico (300 req/s por 30s)"
    )

    # Op√ß√µes avan√ßadas
    parser.add_argument(
        "--batch-size", "-b",
        type=int,
        help="Tamanho do lote para processamento paralelo"
    )

    parser.add_argument(
        "--concurrent", "-c",
        type=int,
        help="M√°ximo de requisi√ß√µes concorrentes"
    )

    parser.add_argument(
        "--monitor", "-m",
        action="store_true",
        help="Ativar monitoramento avan√ßado de performance"
    )

    parser.add_argument(
        "--export-metrics",
        action="store_true",
        help="Exportar m√©tricas detalhadas"
    )

    # Output e logging
    parser.add_argument(
        "--output", "-o",
        type=str,
        help="Arquivo de sa√≠da para o relat√≥rio (JSON)"
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Output verboso"
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Apenas mostrar configura√ß√£o sem executar"
    )

    return parser

def build_config_from_args(args) -> StressTestConfig:
    """Constr√≥i configura√ß√£o baseada nos argumentos"""

    # Come√ßar com configura√ß√£o do ambiente
    config = get_config_for_environment(args.environment)

    # Aplicar cen√°rio se especificado
    if args.scenario:
        scenario = get_scenario_config(args.scenario)
        config.total_users = scenario["users"]
        config.users_per_second = scenario["users_per_second"]

    # Aplicar testes r√°pidos
    if args.quick:
        config.total_users = 100
        config.users_per_second = 20
        config.max_test_duration = 60

    if args.spike:
        config.total_users = 300
        config.users_per_second = 300
        config.max_test_duration = 30

    # Aplicar argumentos espec√≠ficos (sobrescrevem cen√°rios)
    if args.users:
        config.total_users = args.users

    if args.rps:
        config.users_per_second = args.rps

    if args.webhook:
        config.webhook_url = args.webhook

    if args.duration:
        config.max_test_duration = args.duration

    if args.batch_size:
        config.batch_size = args.batch_size

    if args.concurrent:
        config.max_concurrent_requests = args.concurrent

    return config

def print_test_banner():
    """Imprime banner do teste"""
    print("\n" + "="*80)
    print("TESTE DE STRESS - BOT TELEGRAM VIP")
    print("="*80)
    print("ATENCAO: Este teste pode gerar alta carga no servidor!")
    print("Monitor performance em tempo real para detectar problemas")
    print("Use Ctrl+C para interromper o teste a qualquer momento")
    print("="*80 + "\n")

async def run_test_with_monitoring(config: StressTestConfig, enable_monitoring: bool = True) -> dict:
    """Executa teste com monitoramento opcional"""

    if enable_monitoring:
        print("üîç Iniciando monitoramento de performance...")
        start_monitoring()

    try:
        async with StressTester(config) as tester:
            report = await tester.run_stress_test()

            if enable_monitoring:
                # Adicionar m√©tricas de performance ao relat√≥rio
                perf_summary = get_performance_summary()
                report["advanced_monitoring"] = perf_summary

            return report

    finally:
        if enable_monitoring:
            stop_monitoring()

def analyze_results(report: dict) -> dict:
    """Analisa resultados e gera insights"""
    analysis = {
        "overall_grade": "Unknown",
        "performance_issues": [],
        "bottlenecks": [],
        "optimization_suggestions": []
    }

    if "test_summary" not in report:
        return analysis

    summary = report["test_summary"]
    response_times = report.get("response_times", {})

    # Calcular nota geral
    success_rate = summary.get("success_rate_percent", 0)
    avg_response_time = response_times.get("average_ms", 0) / 1000  # converter para segundos
    throughput = summary.get("requests_per_second", 0)

    # Sistema de pontua√ß√£o
    score = 0

    # Pontua√ß√£o por taxa de sucesso
    if success_rate >= PERFORMANCE_THRESHOLDS["success_rate"]["excellent"]:
        score += 40
    elif success_rate >= PERFORMANCE_THRESHOLDS["success_rate"]["good"]:
        score += 30
    elif success_rate >= PERFORMANCE_THRESHOLDS["success_rate"]["acceptable"]:
        score += 20
    else:
        score += 10

    # Pontua√ß√£o por tempo de resposta
    if avg_response_time <= PERFORMANCE_THRESHOLDS["response_time"]["excellent"]:
        score += 30
    elif avg_response_time <= PERFORMANCE_THRESHOLDS["response_time"]["good"]:
        score += 25
    elif avg_response_time <= PERFORMANCE_THRESHOLDS["response_time"]["acceptable"]:
        score += 15
    else:
        score += 5

    # Pontua√ß√£o por throughput
    if throughput >= PERFORMANCE_THRESHOLDS["throughput"]["excellent"]:
        score += 30
    elif throughput >= PERFORMANCE_THRESHOLDS["throughput"]["good"]:
        score += 25
    elif throughput >= PERFORMANCE_THRESHOLDS["throughput"]["acceptable"]:
        score += 15
    else:
        score += 5

    # Determinar nota
    if score >= 85:
        analysis["overall_grade"] = "A+ (Excelente)"
    elif score >= 75:
        analysis["overall_grade"] = "A (Muito Bom)"
    elif score >= 65:
        analysis["overall_grade"] = "B (Bom)"
    elif score >= 50:
        analysis["overall_grade"] = "C (Regular)"
    else:
        analysis["overall_grade"] = "D (Necessita Melhorias)"

    # Identificar problemas espec√≠ficos
    if success_rate < 95:
        analysis["performance_issues"].append(f"Taxa de sucesso baixa ({success_rate:.1f}%)")

    if avg_response_time > 2.0:
        analysis["performance_issues"].append(f"Tempo de resposta alto ({avg_response_time:.2f}s)")
        analysis["bottlenecks"].append("Poss√≠vel gargalo em queries de banco de dados")

    if throughput < 20:
        analysis["performance_issues"].append(f"Throughput baixo ({throughput:.1f} req/s)")
        analysis["bottlenecks"].append("Poss√≠vel limita√ß√£o de concorr√™ncia")

    # Sugest√µes de otimiza√ß√£o baseadas nos resultados
    if len(analysis["performance_issues"]) > 0:
        analysis["optimization_suggestions"].extend([
            "Implementar cache Redis para dados frequentemente acessados",
            "Otimizar queries de banco com indices apropriados",
            "Usar connection pooling para banco de dados",
            "Implementar rate limiting inteligente",
            "Considerar arquitetura assincrona com filas"
        ])

    return analysis

def print_final_report(report: dict, analysis: dict, config: StressTestConfig):
    """Imprime relat√≥rio final formatado"""
    print("\n" + "="*80)
    print("RELATORIO FINAL DO TESTE DE STRESS")
    print("="*80)

    # Resumo executivo
    print(f"\nRESUMO EXECUTIVO:")
    print(f"   Nota Geral: {analysis['overall_grade']}")

    if "test_summary" in report:
        summary = report["test_summary"]
        print(f"   Taxa de Sucesso: {summary.get('success_rate_percent', 0):.1f}%")
        print(f"   Requests/Segundo: {summary.get('requests_per_second', 0):.1f}")
        print(f"   Dura√ß√£o Total: {summary.get('test_duration_seconds', 0):.1f}s")

    # M√©tricas detalhadas
    if "response_times" in report:
        rt = report["response_times"]
        print(f"\nTEMPOS DE RESPOSTA:")
        print(f"   M√©dio: {rt.get('average_ms', 0):.0f}ms")
        print(f"   P95: {rt.get('p95_ms', 0):.0f}ms")
        print(f"   M√°ximo: {rt.get('maximum_ms', 0):.0f}ms")

    # Problemas identificados
    if analysis["performance_issues"]:
        print(f"\nPROBLEMAS IDENTIFICADOS:")
        for issue in analysis["performance_issues"]:
            print(f"   ‚Ä¢ {issue}")

    # Gargalos
    if analysis["bottlenecks"]:
        print(f"\nüîç POSS√çVEIS GARGALOS:")
        for bottleneck in analysis["bottlenecks"]:
            print(f"   ‚Ä¢ {bottleneck}")

    # Recomenda√ß√µes
    if "recommendations" in report:
        print(f"\nRECOMENDACOES:")
        for rec in report["recommendations"][:5]:  # Top 5
            print(f"   {rec}")

    print("\n" + "="*80)

async def main():
    """Fun√ß√£o principal"""
    parser = create_argument_parser()
    args = parser.parse_args()

    # Banner inicial
    print_test_banner()

    # Construir configura√ß√£o
    config = build_config_from_args(args)

    # Mostrar configura√ß√£o
    print_config_summary(config)

    # Validar configura√ß√£o
    warnings = validate_config(config)
    if warnings:
        print(f"\nAVISOS DE CONFIGURACAO:")
        for warning in warnings:
            print(f"   {warning}")

    # Dry run - apenas mostrar configura√ß√£o
    if args.dry_run:
        print("\nüèÉ DRY RUN - Configura√ß√£o validada, n√£o executando teste")
        return

    # Confirma√ß√£o do usu√°rio
    if args.environment == "production":
        print(f"\nATENCAO: Executando em PRODUCAO!")
        confirm = input("Digite 'CONFIRMO' para continuar: ")
        if confirm != "CONFIRMO":
            print("Teste cancelado")
            return

    elif not args.quick:
        print(f"\nATENCAO: Este teste enviara {config.total_users:,} requisicoes")
        confirm = input("Pressione ENTER para continuar ou Ctrl+C para cancelar...")

    print(f"\nIniciando teste de stress...")
    print(f"Monitoramento: {'Ativado' if args.monitor else 'Basico'}")

    start_time = time.time()

    try:
        # Executar teste
        report = await run_test_with_monitoring(config, args.monitor)

        # Analisar resultados
        analysis = analyze_results(report)

        # Mostrar relat√≥rio
        print_final_report(report, analysis, config)

        # Salvar relat√≥rio se solicitado
        if args.output:
            output_file = args.output
        else:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_file = f"stress_test_report_{timestamp}.json"

        # Adicionar an√°lise ao relat√≥rio
        report["analysis"] = analysis
        report["test_config"] = {
            "total_users": config.total_users,
            "users_per_second": config.users_per_second,
            "environment": args.environment,
            "scenario": args.scenario
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ Relat√≥rio completo salvo em: {output_file}")

        # Exportar m√©tricas se solicitado
        if args.export_metrics and args.monitor:
            metrics_file = output_file.replace('.json', '_metrics.json')
            perf_data = get_performance_summary()
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(perf_data, f, indent=2, ensure_ascii=False)
            print(f"Metricas detalhadas em: {metrics_file}")

    except KeyboardInterrupt:
        print(f"\nTeste interrompido pelo usuario")
        print(f"Tempo decorrido: {time.time() - start_time:.1f}s")

    except Exception as e:
        print(f"\nErro durante execucao: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

    print(f"\nTeste concluido em {time.time() - start_time:.1f}s")
    return 0

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nüëã Saindo...")
        sys.exit(0)